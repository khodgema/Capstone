{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377166e6-5ba4-4e4f-b46a-afe77b795915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import ShotChartDetail, LeagueGameLog, BoxScorePlayerTrackV3, BoxScoreAdvancedV3, playbyplayv3, SynergyPlayTypes, BoxScoreTraditionalV3\n",
    "import pandas as pd\n",
    "import time\n",
    "#seasons = ['2020-21','2021-22','2022-23','2023-24']\n",
    "#seasons = ['2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19', '2019-20','2020-21','2021-22','2022-23','2023-24']\n",
    "seasons = ['2024-25']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ad7f2-d5bf-47ea-b1ac-74ee4ff583eb",
   "metadata": {},
   "source": [
    "# Data Fetching\n",
    "Data gathered through the NBA API with the basic pattern for most endpoints being get the games in a season and query the endpoint for each game. Play type data is not done on a game by game basis so is retrived sepratly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b535f327-6a8c-45d8-841d-07d8a4c610e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_ids(season, season_type='Regular Season'):\n",
    "    game_log = LeagueGameLog(season=season, season_type_all_star=season_type).get_data_frames()[0]\n",
    "    game_ids = game_log['GAME_ID'].unique().tolist()\n",
    "    return game_ids\n",
    "\n",
    "def get_data(game_ids):\n",
    "    play_by_play_data = []\n",
    "    shotchart_data = []\n",
    "    playertracking_data = []\n",
    "    advanced_data = []\n",
    "    box_data = []\n",
    "    \n",
    "    for game_id in game_ids:\n",
    "        try:\n",
    "            #standard box\n",
    "            try:\n",
    "                box = BoxScoreTraditionalV3(game_id = game_id).get_data_frames()[0]\n",
    "                box['GAME_ID'] = game_id\n",
    "                box_data.append(box)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed box traditional for {game_id}: {e}\")\n",
    "                time.sleep(1)\n",
    "            \n",
    "            #play by play\n",
    "            try:\n",
    "                play_by_play = playbyplayv3.PlayByPlayV3(game_id=game_id).get_data_frames()[0]\n",
    "                play_by_play['GAME_ID'] = game_id\n",
    "                play_by_play_data.append(play_by_play)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed PlayByPlayV2 for {game_id}: {e}\")\n",
    "                time.sleep(1)\n",
    "                \n",
    "            #ShotChartDetail\n",
    "            try:\n",
    "                shotchart = ShotChartDetail(game_id_nullable=game_id, team_id=0, player_id=0,context_measure_simple='FGA').get_data_frames()[0]\n",
    "                shotchart['GAME_ID'] = game_id\n",
    "                shotchart_data.append(shotchart)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed ShotChartDetail for {game_id}: {e}\")\n",
    "                time.sleep(1) \n",
    "            \n",
    "            #BoxScorePlayerTrackV3\n",
    "            try:\n",
    "                player_tracking = BoxScorePlayerTrackV3(game_id=game_id).get_data_frames()[0]\n",
    "                player_tracking['GAME_ID'] = game_id\n",
    "                playertracking_data.append(player_tracking)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed BoxScorePlayerTrackV3 for {game_id}: {e}\")\n",
    "                time.sleep(1)\n",
    "            \n",
    "            #BoxScoreAdvancedV3\n",
    "            try:\n",
    "                advanced_boxscore = BoxScoreAdvancedV3(game_id=game_id).get_data_frames()[0]\n",
    "                advanced_boxscore['GAME_ID'] = game_id\n",
    "                advanced_data.append(advanced_boxscore)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed BoxScoreAdvancedV3 for {game_id}: {e}\")\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # rate limiting\n",
    "            time.sleep(0.6)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for game {game_id}: {e}\")\n",
    "            time.sleep(10) \n",
    "    \n",
    "    # Concat dataframes only if data exists\n",
    "    shotchart_df = pd.concat(shotchart_data, ignore_index=True) if shotchart_data else pd.DataFrame()\n",
    "    playertracking_df = pd.concat(playertracking_data, ignore_index=True) if playertracking_data else pd.DataFrame()\n",
    "    advanced_df = pd.concat(advanced_data, ignore_index=True) if advanced_data else pd.DataFrame()\n",
    "    play_by_play_df = pd.concat(play_by_play_data, ignore_index=True) if play_by_play_data else pd.DataFrame()\n",
    "    traditional_box_df = pd.concat(box_data,ignore_index=True) if box_data else pd.DataFrame()\n",
    "    \n",
    "    return shotchart_df, playertracking_df, advanced_df,play_by_play_df, traditional_box_df\n",
    "\n",
    "for season in seasons:\n",
    "    game_ids = get_game_ids(season = season)\n",
    "\n",
    "\n",
    "    shotchart_df, playertracking_df,advanced_box_df,play_by_play_df,traditional_box_df = get_data(game_ids)\n",
    "    \n",
    "    #names\n",
    "    shotchart_file = f\"{season}_shotchart_data.csv\"\n",
    "    playertracking_file = f\"{season}_playertracking_data.csv\"\n",
    "    advanced_box_file = f\"{season}_advanced_box_data.csv\"\n",
    "    play_by_play_file = f'{season}_play_by_play.csv'\n",
    "    traditional_boxfile = f\"{season}_box_data.csv\"\n",
    "\n",
    "    \n",
    "    shotchart_df.to_csv(shotchart_file, index=False)\n",
    "    playertracking_df.to_csv(playertracking_file, index=False)\n",
    "    advanced_box_df.to_csv(advanced_box_file,index=False)\n",
    "    play_by_play_df.to_csv(play_by_play_file, index=False)\n",
    "    traditional_box_df.to_csv(traditional_boxfile, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a017703-a978-4be4-87eb-7bcece01efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
      "/tmp/ipykernel_506292/233088658.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n"
     ]
    }
   ],
   "source": [
    "league_id = '00'  # NBA\n",
    "season_type = 'Regular Season'\n",
    "per_mode = 'PerGame'\n",
    "player_or_team = 'P'\n",
    "play_types = ['Cut', 'Handoff', 'Isolation', 'Misc', 'OffScreen', 'Postup', 'PRBallHandler', 'PRRollman', 'OffRebound', 'Spotup', 'Transition']\n",
    "type_groupings = ['offensive', 'defensive']\n",
    "\n",
    "for season in seasons:\n",
    "    synergy_data = [] \n",
    "    \n",
    "    for type_grouping in type_groupings:\n",
    "        for play_type in play_types:\n",
    "            try:\n",
    "                # Fetch the synergy data\n",
    "                synergy = SynergyPlayTypes(\n",
    "                    league_id=league_id,\n",
    "                    per_mode_simple=per_mode,\n",
    "                    player_or_team_abbreviation=player_or_team,\n",
    "                    season_type_all_star=season_type,\n",
    "                    season=season,\n",
    "                    play_type_nullable=play_type,\n",
    "                    type_grouping_nullable=type_grouping\n",
    "                ).get_data_frames()[0]\n",
    "                \n",
    "                synergy_data.append(synergy)\n",
    "                time.sleep(0.6)  # Delay for rate limiting\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed Synergy for season: {season}, play type: {play_type}, type grouping: {type_grouping}: {e}\")\n",
    "                time.sleep(1)  # Delay before retrying on error\n",
    "\n",
    "    \n",
    "    synergy_df = pd.concat(synergy_data, ignore_index=True) if synergy_data else pd.DataFrame()\n",
    "\n",
    "    # Save\n",
    "    play_type_file = f'{season}_play_type.csv'\n",
    "    synergy_df.to_csv(play_type_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f84e8825-80d7-4a27-83f3-cea98a377721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BoxScoreTraditional': ['0021500241', '0021700370'], 'PlayByPlay': ['0022100301', '0022100506', '0022200707', '0021500192', '0021500242', '0021700079', '0021700319', '0021700370', '0021800264', '0021900799'], 'ShotChartDetail': ['0021700370', '0021700916', '0021900588'], 'BoxScorePlayerTrack': ['0021700370', '0021700558'], 'BoxScoreAdvanced': ['0021700120']}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_failed_ids(asdf):\n",
    "    failed_ids = {\n",
    "        \"BoxScoreTraditional\": [],\n",
    "        \"PlayByPlay\": [],\n",
    "        \"ShotChartDetail\": [],\n",
    "        \"BoxScorePlayerTrack\": [],\n",
    "        \"BoxScoreAdvanced\": []\n",
    "    }\n",
    "    \n",
    "    # Define regex patterns for each category\n",
    "    patterns = {\n",
    "        \"BoxScoreTraditional\": r\"Failed box traditional for (\\d+):\",\n",
    "        \"PlayByPlay\": r\"Failed PlayByPlayV2 for (\\d+):\",\n",
    "        \"ShotChartDetail\": r\"Failed ShotChartDetail for (\\d+):\",\n",
    "        \"BoxScorePlayerTrack\": r\"Failed BoxScorePlayerTrackV3 for (\\d+):\",\n",
    "        \"BoxScoreAdvanced\": r\"Failed BoxScoreAdvancedV3 for (\\d+):\"\n",
    "    }\n",
    "    \n",
    "    for category, pattern in patterns.items():\n",
    "        matches = re.findall(pattern, asdf)\n",
    "        failed_ids[category].extend(matches)\n",
    "    \n",
    "    return failed_ids\n",
    "\n",
    "\n",
    "failed_ids = parse_failed_ids(asdf)\n",
    "print(failed_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd8586b4-724e-4c37-89e2-fe840e1987c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for BoxScoreTraditional game ID 0021500241\n",
      "Appended data for game ID 0021500241\n",
      "Fetching data for BoxScoreTraditional game ID 0021700370\n",
      "Appended data for game ID 0021700370\n",
      "Updated file saved: 2021_boxscoretraditional_data.csv\n",
      "Fetching data for PlayByPlay game ID 0022100301\n",
      "Appended data for game ID 0022100301\n",
      "Fetching data for PlayByPlay game ID 0022100506\n",
      "Appended data for game ID 0022100506\n",
      "Fetching data for PlayByPlay game ID 0022200707\n",
      "Appended data for game ID 0022200707\n",
      "Fetching data for PlayByPlay game ID 0021500192\n",
      "Appended data for game ID 0021500192\n",
      "Fetching data for PlayByPlay game ID 0021500242\n",
      "Appended data for game ID 0021500242\n",
      "Fetching data for PlayByPlay game ID 0021700079\n",
      "Appended data for game ID 0021700079\n",
      "Fetching data for PlayByPlay game ID 0021700319\n",
      "Appended data for game ID 0021700319\n",
      "Fetching data for PlayByPlay game ID 0021700370\n",
      "Appended data for game ID 0021700370\n",
      "Fetching data for PlayByPlay game ID 0021800264\n",
      "Appended data for game ID 0021800264\n",
      "Fetching data for PlayByPlay game ID 0021900799\n",
      "Appended data for game ID 0021900799\n",
      "Updated file saved: 2022_play_by_play.csv\n",
      "Fetching data for ShotChartDetail game ID 0021700370\n",
      "Attempt 1 failed for game 0021700370: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Attempt 2 failed for game 0021700370: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Attempt 3 failed for game 0021700370: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Failed after 3 attempts for game 0021700370\n",
      "No data fetched for game ID 0021700370\n",
      "Fetching data for ShotChartDetail game ID 0021700916\n",
      "Attempt 1 failed for game 0021700916: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Attempt 2 failed for game 0021700916: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Attempt 3 failed for game 0021700916: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Failed after 3 attempts for game 0021700916\n",
      "No data fetched for game ID 0021700916\n",
      "Fetching data for ShotChartDetail game ID 0021900588\n",
      "Attempt 1 failed for game 0021900588: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Attempt 2 failed for game 0021900588: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Attempt 3 failed for game 0021900588: ShotChartDetail.__init__() got an unexpected keyword argument 'game_id'\n",
      "Failed after 3 attempts for game 0021900588\n",
      "No data fetched for game ID 0021900588\n",
      "Updated file saved: 2021_shotchart_data.csv\n",
      "Fetching data for BoxScorePlayerTrack game ID 0021700370\n",
      "Appended data for game ID 0021700370\n",
      "Fetching data for BoxScorePlayerTrack game ID 0021700558\n",
      "Appended data for game ID 0021700558\n",
      "Updated file saved: 2021_boxscoreplayertrack_data.csv\n",
      "Fetching data for BoxScoreAdvanced game ID 0021700120\n",
      "Appended data for game ID 0021700120\n",
      "Updated file saved: 2021_advanced_box_data.csv\n"
     ]
    }
   ],
   "source": [
    "#GPT\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "# Define the fetch functions for each category\n",
    "fetch_functions = {\n",
    "    \"BoxScoreTraditional\": BoxScoreTraditionalV3,\n",
    "    \"PlayByPlay\": playbyplayv3.PlayByPlayV3,\n",
    "    \"ShotChartDetail\": ShotChartDetail,\n",
    "    \"BoxScorePlayerTrack\": BoxScorePlayerTrackV3,\n",
    "    \"BoxScoreAdvanced\": BoxScoreAdvancedV3\n",
    "}\n",
    "\n",
    "# Determine the season from the game ID\n",
    "def get_season_from_game_id(game_id):\n",
    "    year_prefix = game_id[:4]  # Extract the first four characters (e.g., \"0022\" for 2022)\n",
    "    return f\"{int(year_prefix) + 2000}\"  # Adjust to the format (e.g., \"2022\")\n",
    "\n",
    "# Function to fetch data with retries\n",
    "def fetch_data_with_retries(fetch_function, game_id, **kwargs):\n",
    "    for attempt in range(3):  # Number of retries\n",
    "        try:\n",
    "            data = fetch_function(game_id=game_id, **kwargs).get_data_frames()[0]\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for game {game_id}: {e}\")\n",
    "            sleep(1)  # Delay between attempts\n",
    "    print(f\"Failed after 3 attempts for game {game_id}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Update the data files with missing data\n",
    "def update_data_files(failed_ids):\n",
    "    for category, game_ids in failed_ids.items():\n",
    "        file_name = {\n",
    "            \"BoxScoreTraditional\": \"boxscoretraditional_data.csv\",\n",
    "            \"PlayByPlay\": \"play_by_play.csv\",\n",
    "            \"ShotChartDetail\": \"shotchart_data.csv\",\n",
    "            \"BoxScorePlayerTrack\": \"boxscoreplayertrack_data.csv\",\n",
    "            \"BoxScoreAdvanced\": \"advanced_box_data.csv\"\n",
    "        }.get(category, None)\n",
    "\n",
    "        if not file_name:\n",
    "            print(f\"Unknown category: {category}\")\n",
    "            continue\n",
    "\n",
    "        # Determine the season based on the game IDs\n",
    "        first_game_id = game_ids[0]  # Use the first game ID to determine the season\n",
    "        season = get_season_from_game_id(first_game_id)\n",
    "        file_path = f\"{season}_{file_name}\"\n",
    "\n",
    "        # Load existing data if the file exists\n",
    "        existing_df = pd.read_csv(file_path) if os.path.exists(file_path) else pd.DataFrame()\n",
    "\n",
    "        for game_id in game_ids:\n",
    "            print(f\"Fetching data for {category} game ID {game_id}\")\n",
    "            if category == \"BoxScoreTraditional\":\n",
    "                new_data = fetch_data_with_retries(fetch_functions[category], game_id)\n",
    "            elif category == \"PlayByPlay\":\n",
    "                new_data = fetch_data_with_retries(fetch_functions[category], game_id)\n",
    "            elif category == \"ShotChartDetail\":\n",
    "                new_data = fetch_data_with_retries(fetch_functions[category], game_id, team_id=0, player_id=0, context_measure_simple='FGA')\n",
    "            elif category == \"BoxScorePlayerTrack\":\n",
    "                new_data = fetch_data_with_retries(fetch_functions[category], game_id)\n",
    "            elif category == \"BoxScoreAdvanced\":\n",
    "                new_data = fetch_data_with_retries(fetch_functions[category], game_id)\n",
    "            else:\n",
    "                print(f\"Unknown category: {category}\")\n",
    "                continue\n",
    "\n",
    "            if not new_data.empty:\n",
    "                # Append new data to the existing DataFrame\n",
    "                existing_df = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "                print(f\"Appended data for game ID {game_id}\")\n",
    "            else:\n",
    "                print(f\"No data fetched for game ID {game_id}\")\n",
    "\n",
    "            # Optional: Avoid hitting rate limits\n",
    "            sleep(1)\n",
    "\n",
    "        # Save the updated DataFrame back to CSV\n",
    "        existing_df.to_csv(file_path, index=False)\n",
    "        print(f\"Updated file saved: {file_path}\")\n",
    "\n",
    "update_data_files(failed_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00ee5dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data. HTTP Status Code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m schedule_url \u001b[38;5;241m=\u001b[39m base_data\u001b[38;5;241m.\u001b[39mget(base_url)  \u001b[38;5;66;03m# Replace 'link' with the actual key that stores the URL.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m schedule_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(schedule_url)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schedule_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     15\u001b[0m     schedule_data \u001b[38;5;241m=\u001b[39m schedule_response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/requests/sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[1;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[1;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[1;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[1;32m    581\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/requests/sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    481\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    483\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[0;32m--> 484\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m    485\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    486\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    487\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[1;32m    488\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m    489\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[1;32m    490\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[1;32m    491\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[1;32m    492\u001b[0m     ),\n\u001b[1;32m    493\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[1;32m    494\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[1;32m    495\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[1;32m    496\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/requests/models.py:367\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_url(url, params)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/requests/models.py:438\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;241m*\u001b[39me\u001b[38;5;241m.\u001b[39margs)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No host supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "base_url = \"https://cdn.nba.com/static/json/staticData/scheduleLeagueV2.json\"\n",
    "\n",
    "response = requests.get(base_url)\n",
    "if response.status_code == 200:\n",
    "    base_data = response.json()\n",
    "else:\n",
    "    print(f\"Failed to fetch data. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "schedule_url = base_data.get(base_url)  # Replace 'link' with the actual key that stores the URL.\n",
    "\n",
    "schedule_response = requests.get(schedule_url)\n",
    "if schedule_response.status_code == 200:\n",
    "    schedule_data = schedule_response.json()\n",
    "else:\n",
    "    print(f\"Failed to fetch schedule. HTTP Status Code: {schedule_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b07b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# URL for NBA Stats API\n",
    "url = \"https://stats.nba.com/stats/scheduleleaguev2\"\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    \"Season\": \"2024-25\",\n",
    "    \"LeagueID\": \"00\",\n",
    "}\n",
    "\n",
    "# Headers to mimic a browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.nba.com/\",\n",
    "    \"Origin\": \"https://www.nba.com\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "# Check for successful response\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17427a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the games data\n",
    "game_dates = data['leagueSchedule']['gameDates']\n",
    "all_games = []\n",
    "\n",
    "for date_info in game_dates:\n",
    "    game_date = date_info['gameDate']\n",
    "    games = date_info['games']\n",
    "    for game in games:\n",
    "        game['gameDate'] = game_date  # Add the date to each game record\n",
    "        all_games.append(game)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_games)\n",
    "\n",
    "\n",
    "# Parse gameDate into datetime objects\n",
    "df['gameDate'] = pd.to_datetime(df['gameDate'], format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "# Define current date and one week ahead\n",
    "current_date = datetime.utcnow()\n",
    "next_week = current_date + timedelta(weeks=1)\n",
    "\n",
    "# Filter games for the next week\n",
    "next_week_games = df[(df['gameDate'] >= current_date) & (df['gameDate'] <= next_week)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a703a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gameId', 'gameCode', 'gameStatus', 'gameStatusText', 'gameSequence',\n",
       "       'gameDateEst', 'gameTimeEst', 'gameDateTimeEst', 'gameDateUTC',\n",
       "       'gameTimeUTC', 'gameDateTimeUTC', 'awayTeamTime', 'homeTeamTime', 'day',\n",
       "       'monthNum', 'weekNumber', 'weekName', 'ifNecessary', 'seriesGameNumber',\n",
       "       'gameLabel', 'gameSubLabel', 'seriesText', 'arenaName', 'arenaState',\n",
       "       'arenaCity', 'postponedStatus', 'branchLink', 'gameSubtype',\n",
       "       'broadcasters', 'homeTeam', 'awayTeam', 'pointsLeaders', 'gameDate',\n",
       "       'homeTeamName'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0314ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_teamId</th>\n",
       "      <th>away_teamId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1610612743</td>\n",
       "      <td>1610612738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1610612762</td>\n",
       "      <td>15020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1610612747</td>\n",
       "      <td>1610612750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1610612746</td>\n",
       "      <td>1610612744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1610612738</td>\n",
       "      <td>1610612743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>1610612740</td>\n",
       "      <td>1610612760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>1610612759</td>\n",
       "      <td>1610612761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>1610612744</td>\n",
       "      <td>1610612746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>1610612757</td>\n",
       "      <td>1610612747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>1610612758</td>\n",
       "      <td>1610612756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1281 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      home_teamId  away_teamId\n",
       "0      1610612743   1610612738\n",
       "1      1610612762        15020\n",
       "2      1610612747   1610612750\n",
       "3      1610612746   1610612744\n",
       "4      1610612738   1610612743\n",
       "...           ...          ...\n",
       "1276   1610612740   1610612760\n",
       "1277   1610612759   1610612761\n",
       "1278   1610612744   1610612746\n",
       "1279   1610612757   1610612747\n",
       "1280   1610612758   1610612756\n",
       "\n",
       "[1281 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "home = pd.json_normalize(df['homeTeam'])\n",
    "away = pd.json_normalize(df['awayTeam'])\n",
    "\n",
    "teams_combined = pd.concat(\n",
    "    [home[['teamId']].rename(columns={'teamId': 'home_teamId'}),\n",
    "     away[['teamId']].rename(columns={'teamId': 'away_teamId'})],\n",
    "    axis=1\n",
    ")\n",
    "teams_combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
